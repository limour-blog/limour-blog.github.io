---
title: 【迁移】菜鸟博主对神经网络的简单理解
urlname: simple-understanding-of-neural-networks
index_img: https://api.limour.top/randomImg?d=2024-01-16 05:25:22
date: 2022-07-28 13:25:22
tags: llama
---
博主最近在看临床预测模型，里面涉及到一些神经网络的相关知识，这里记录一下博主的简单理解。

## 基本概念

*   `X`：一个nx维的输入张量，可以是nx=0的标量，nx=1的向量，nx=2的矩阵，或更高维的张量
*   `Y`：一个ny维的输出张量，可以是nx=0的标量，nx=1的向量，nx=2的矩阵，或更高维的张量
*   `f`：一个特定结构的[神经网络](https://zhuanlan.zhihu.com/p/159305118)，如简单的BP神经网络、复杂的深度神经网络**DNN**等，用于将X映射到Y
*   `W`：一个nw维的张量，用来代表f中的所有权重，nw与f、X、Y有关，具体多少不用管
*   `L`：损失函数，可以是交叉熵、残差平方等，用来计算f(X)与实际Y的差距

一个简单的例子：假设`f`是DNN，`X`是一个256\*256图片的像素矩阵，`Y`是一个3维分类向量，那么`f`就可以用于将一张特定的256\*256图片`X0`映射到一个特定的`Y0(P(🐱), P(🐕), P(🐖))`，那么我们输出`(🐱,🐕,🐖)[which.max(Y0)]`，就可以对图片`X0`进行`(🐱,🐕,🐖)`的分类。

## 对训练的理解

假设我们有一个训练好的权重`W`，那么我们就可以得到任意`X`下对实际`Y`的良好估计`f(X,W)`,有`L(f(X,W),Y)`相对较小。我们训练的目的，就是根据已有的数据集`D{(X,Y)}`，通过适当的拟合来找到这样一个权重`W`，它可以实现前面假设中的效果。

那么如何进行适当的拟合呢？一个简单的思路是梯度下降法。根据定义，我们知道`L`是`f(X)`和`Y`的函数，那么变换视角，`L`就是关于`W`的函数。假设学习率为`η`，初始权重为`W0`，对于`D`中的任意`(X0,Y0)`，我们可以求`ΔW0=-η∇L(W0,(X0,Y0),f)`。根据梯度的特性，我们可以知道对于更新后的权重`W1=W0+ΔW0`，有`L(W1,(X0,Y0),f)<L(W0,(X0,Y0),f)`。在适当的参数下，通过不断循环上述过程，就能得到最终的`W`。

那么`∇L(W,f)`如何求呢？作为医学生，我们不必了解具体的数学原理来编程，只要构建好我们想要的网络结构`f`，[PyTorch](https://pytorch.org/)的[自动求导](https://pytorch.org/docs/stable/autograd.html)功能就可以自动帮我们计算出`∇L(W,f)`啦。