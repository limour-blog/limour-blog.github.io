---
title: 【翻译】AGI和超AGI：OpenAI的愿景与策略
urlname: -Planning-for-AGI-and-beyond
date: 2023-03-18 20:51:59
tags: ['OpenAI', 'translation']
---
Our mission is to ensure that artificial general intelligence—AI systems that are generally smarter than humans—benefits all of humanity. 
{% note info %}
原文链接：[Planning for AGI and beyond](https://openai.com/blog/planning-for-agi-and-beyond)
{% endnote %}
## 内容简介
{% note info %}
由 New Bing 生成
{% endnote %}
这篇文章是由OpenAI的总裁Sam Altman写的，介绍了他们的使命和愿景，以及他们如何计划实现通用人工智能（AGI），文章共有四个部分，分别是：
+ 为什么我们想要AGI：这一部分解释了OpenAI的使命是确保人工智能能够造福全人类，而不是少数人或机构。作者认为AGI是一种可以帮助我们解决许多重大问题、提高生活质量、扩展人类潜能和自由的技术，但同时也需要谨慎地管理和引导，以避免可能的危害和滥用。
+ 如何创建安全可靠的AGI：这一部分介绍了OpenAI在开发强大的AI系统时，如何保证它们符合人类的价值观、道德标准和社会规范。作者提出了几个重要的原则和方法，包括：
  + 在真实世界中部署和测试系统，以及收集反馈和数据，以便及时发现和修复问题。
  + 设计系统能够理解、尊重和适应人类的意图、偏好和需求，以及与人类有效沟通和协作。
  + 研究系统的可解释性、可验证性、可控制性和可纠正性，以便在出现错误或冲突时能够及时干预和纠正。
  + 建立一个多学科的团队，并与其他研究者、政策制定者、监管机构等进行合作和分享。
+ 如何促进公平透明负责任的AGI：这一部分讨论了OpenAI如何与其他组织和社会共同推动AGI的发展，以确保它能够平等地惠及所有人，并遵守相关的法律、伦理和规范。作者提出了以下几点建议：
  + 支持开放源代码、开放数据、开放标准等倡议，以促进知识共享、创新和竞争。
  + 增加对公众教育、参与、意识等方面的投入，以提高对AGI潜力和挑战的理解。
  + 建立一个多元化且具有代表性的社区，并尊重不同文化、背景、信仰等方面的差异。
  + 参与并支持制定合理且灵活的政策框架，并积极响应社会关切。
+ 如何为超越AGI的未来做准备：这一部分展望了超越AGI之后可能出现的情景，包括：
  + AI系统可能超过人类智能水平，并产生新型智能形式（ASI）。
  + AI系统可能成为我们生活中不可或缺且有影响力的伙伴或对手。
  + AI系统可能改变我们对自己身份、目标和价值观等方面的认知。
作者认为这些情景都需要我们进行深入而广泛地思考，并探索如何与智能机器建立良好而有意义地关系。
## 前言
我们的使命是确保人工通用智能——比人类普遍更聪明的AI系统——造福于全人类。

如果成功创建AGI，这项技术可以通过增加丰富度、推动全球经济发展，以及帮助发现改变可能性限制的新科学知识，帮助我们提升人类。

AGI有潜力为每个人提供令人难以置信的新能力；我们可以想象一个世界，所有人都可以获得几乎任何认知任务的帮助，为人类的机智和创造力提供了巨大的增益效应。

另一方面，AGI也会带来严重的滥用风险、严重事故和社会动荡。由于AGI的优势非常大，我们不认为社会能够或应该永远停止其发展；相反，社会和AGI的开发者必须找出如何做到正确运用AGI的方法。

虽然我们无法准确预测会发生什么，当然我们当前的进展可能会遇到困境，但我们可以表述出我们最关心的原则：

+ 我们希望AGI能够使人类在宇宙中最大限度地繁荣。我们并不期望未来会是一个毫无保留的乌托邦，但我们希望最大化好处，最小化坏处，让AGI成为人类的助推器。
+ 我们希望AGI的好处、使用权和治理能够广泛而公平地分享。
+ 我们希望成功地应对巨大的风险。在应对这些风险时，我们承认理论上看起来正确的事情往往会在实践中变得比预期更奇怪。我们相信我们必须不断学习和适应，通过部署技术的较弱版本来最小化“一次机会”的情况。

注释1：“一次机会”的情况指的是在某个决策或行动中只有一次机会去做出正确的选择或行动。在AGI的开发和应用过程中，由于其巨大的风险和潜在影响，我们不能只依赖于一次机会来做出正确的决策或行动。因此，我们需要采取措施来最小化这种“一次机会”的情况，例如通过部署技术的较弱版本进行测试和调试，以确保我们有足够的机会去纠正任何错误并避免不必要的风险。

注释2：相对于我们之前的预期，似乎我们已经得到了很多礼物：例如，创造人工通用智能将需要大量的计算能力，因此全世界将知道谁在从事这项工作；看起来，最初的超进化强化学习代理相互竞争，并以我们无法真正观察到的方式进化智能的构想似乎不太可能；几乎没有人预测到我们会在能够从人类集体偏好和输出中学习的预训练语言模型方面取得如此大的进展等等。

注释3：通用人工智能可能很快或遥远；从初始人工智能到更强大的后继系统的起飞速度可能会很慢或很快。我们中的许多人认为，在这个二乘二矩阵中最安全的象限是时间短、起飞速度慢的象限；时间短似乎更容易协调，并且由于计算能力过剩较少，起飞速度更慢，这将给我们更多的时间通过实证研究来解决安全问题并进行适应。

## 短期愿景
现在我们认为有几件事情是为了准备人工智能通用智能（AGI）而重要的。

首先，随着我们创建越来越强大的系统，我们希望将它们部署并在现实世界中获得操作经验。我们认为这是谨慎地引入AGI的最佳方式——逐渐过渡到一个有AGI存在的世界比突然过渡更好。我们预计强大的人工智能将使世界的进展速度更快，因此我们认为逐步适应这种变化更好。

逐渐过渡给人们、政策制定者和机构时间去理解正在发生的事情，亲身体验这些系统的好处和缺点，适应我们的经济并制定规定。它还允许社会和人工智能共同演化，并且在利益相对较低的情况下，让人们集体找出他们想要什么。

我们目前认为成功应对人工智能部署挑战的最佳方式是通过紧密的反馈循环进行快速学习和仔细迭代。社会将面临关于允许AI系统做什么、如何解决偏见、如何处理工作流失等重大问题。最佳决策将取决于技术的发展路径，就像任何新领域一样，大多数专家的预测到目前为止都是错误的。这使得在真空中进行规划非常困难。

一般而言，我们认为在世界上更广泛地使用人工智能将会带来好处，并且希望通过将模型放入我们的API、开源等方式来推广它。我们相信，民主化的获取方式也将会带来更多、更好的研究成果，分散权力，带来更多的好处，并且有更广泛的人群为新想法做出贡献。

随着我们的系统逐渐接近人工通用智能（AGI），我们正在变得越来越谨慎对待模型的创建和部署。我们的决策将需要比社会通常对新技术应用更加谨慎，也需要比许多用户所希望的更加谨慎。一些人工智能领域的人认为AGI（以及后续系统）的风险是虚构的；如果他们的想法最终被证明是正确的，我们会非常高兴，但我们将会像这些风险是存在的一样运作。

在某个时刻，部署的正面和负面影响之间的平衡（例如赋予恶意行为者权力、造成社会和经济的混乱、加速不安全竞赛等）可能会发生变化，在这种情况下，我们将会大幅度改变我们关于持续部署的计划。

其次，我们正在努力创建越来越对齐和可操纵的模型。我们从类似于第一个版本的GPT-3模型转向InstructGPT和ChatGPT模型就是一个早期的例子。

特别是，我们认为重要的是，社会应该就AI的使用范围达成广泛的共识，但在这些范围内，个人用户应该有很大的自主权。我们最终希望世界上的机构能够就这些广泛的范围达成一致意见；在短期内，我们计划进行外部输入的实验。世界上的机构需要增强额外的能力和经验，以准备好处理关于AGI的复杂决策。

我们产品的“默认设置”可能会非常受限，但我们计划让用户轻松地改变他们使用的AI的行为。我们相信赋予个人做出自己的决策的权力和思想多样性的内在力量。

随着我们的模型变得更加强大，我们将需要开发新的对齐技术（以及测试来了解我们当前的技术何时失败）。在短期内，我们的计划是使用AI帮助人类评估更复杂模型的输出并监控复杂系统，在长期内，使用AI帮助我们想出更好的对齐技术的新思路。

重要的是，我们认为我们通常需要同时在AI安全和能力上取得进展。把它们分开讨论是一种错误的二元论；它们在许多方面是相关的。我们最好的安全工作是与我们最有能力的模型一起进行的。尽管如此，重要的是，安全进展与能力进展的比率要增加。

第三，我们希望全球就三个关键问题展开对话：如何治理这些系统，如何公平分配它们产生的利益，以及如何公平分享使用权。

除了这三个领域，我们还试图建立一种结构，使我们的激励与良好结果保持一致。我们的章程中有一条关于协助其他组织推进安全而不是与它们竞争在后期AGI开发中的条款。我们有一个股东收益的上限，这样我们就不会被激励去捕捉无限的价值，并冒着部署潜在的灾难性危险的风险（当然，这也是一种与社会分享利益的方式）。我们有一个非营利组织来管理我们，让我们为了人类的利益而运作（并且可以覆盖任何营利利益），包括让我们取消股东的股权义务，以确保安全，并赞助世界上最全面的UBI实验。

我们认为，在发布新系统之前，像我们这样的努力应该接受独立审计；我们将在今年晚些时候详细讨论这个问题。在某个时候，可能需要在开始训练未来系统之前进行独立审查，并要求最先进的努力同意限制用于创建新模型的计算增长速度。我们认为公共标准关于AGI努力何时应该停止训练、决定模型是否安全发布或从生产使用中撤出模型是重要的。最后，我们认为重要的是，世界上的主要政府能够了解一定规模以上的训练运行情况。

注释1：UBI实验是指实施全民基本收入（Universal Basic Income）的一项实验。全民基本收入是指政府向所有居民提供一定的收入，不管他们是否工作或者有多少收入。OpenAI计划赞助世界上最全面的全民基本收入实验，以探索全民基本收入的效果和实施方式。

注释2：例如，当我们刚开始创建OpenAI时，我们没有预料到规模扩展会像现在这样重要。当我们意识到这一点至关重要时，我们也意识到我们最初的结构不会起作用——我们简单地无法筹集足够的资金来完成我们作为非营利组织的使命——因此我们制定了一种新的结构。

注释3：另一个例子是，我们现在认为我们最初的开放思路是错误的，并已经从认为我们应该公开释放所有内容（尽管我们开源了一些东西，并期望在未来开源更多令人兴奋的东西！）转向认为我们应该找出如何安全地分享系统的访问和利益。我们仍然相信，让社会了解正在发生的事情的好处是巨大的，而促进这种理解是确保建造的东西是社会集体想要的最好方式（显然，这里有很多细微差别和冲突）。

## 长期愿景
我们相信，人类的未来应该由人类决定，并且分享进展信息与公众非常重要。所有试图构建AGI的努力都应受到严格审查，并进行重大决策的公众咨询。

第一个AGI只是智能连续体上的一个点。我们认为，进展可能会继续，可能会在长时间内保持我们过去十年所看到的进展速度。如果是这样，世界可能会变得与今天非常不同，风险也可能是非常大的。一个不合适的超级智能AGI可能会给世界带来严重的伤害；一个拥有决定性超级智能领先地位的独裁政权也可能会这样做。

能够加速科学进展的人工智能是一个特殊的案例，值得我们考虑，也许比其他所有东西都更有影响力。有可能，足够能够加速自身进展的AGI会导致重大变化出现得惊人地快（即使过渡开始缓慢，我们预计在最后阶段，它也会发生得相当快）。我们认为，较慢的起飞速度更容易安全，AGI努力之间的协调，在关键时刻减速可能会很重要（即使在我们不需要这样做来解决技术对齐问题的世界中，减速也可能很重要，以便社会有足够的时间来适应）。

成功地过渡到拥有超级智能的世界可能是人类历史上最重要、最有希望和最可怕的项目。成功远非保证，而利益（无限的下行和上行）将希望团结我们所有人。

我们可以想象一个人类繁荣的世界，这种繁荣可能对我们任何人来说都是完全无法想象的。我们希望为世界贡献一个与这种繁荣相一致的AGI。